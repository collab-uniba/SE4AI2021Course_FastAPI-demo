{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drift Detection using Deepchecks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "training_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "training_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have collected some input data from the production deployment of our model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ../scripts/fake-production-data.py -n 150 -o fake-production-data.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to detect potential drifts, i.e., we want to check if the data is still coming from the same distribution as the data we used to train our model.\n",
    "\n",
    "First, we read in our production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_production_df = pd.read_csv(\"fake-production-data.csv\", header=None)\n",
    "fake_production_df.columns = training_df.columns\n",
    "fake_production_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_production_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the different distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axs[0, 0].hist(training_df[\"sepal length (cm)\"], alpha=0.75, label=\"training\")\n",
    "axs[0, 0].hist(fake_production_df[\"sepal length (cm)\"], alpha=0.50, label=\"production\")\n",
    "axs[0, 0].set_title(\"sepal length (cm)\")\n",
    "axs[0, 0].legend()\n",
    "axs[0, 1].hist(training_df[\"sepal width (cm)\"], alpha=0.75, label=\"training\")\n",
    "axs[0, 1].hist(fake_production_df[\"sepal width (cm)\"], alpha=0.50, label=\"production\")\n",
    "axs[0, 1].set_title(\"sepal width (cm)\")\n",
    "axs[0, 1].legend()\n",
    "axs[1, 0].hist(training_df[\"petal length (cm)\"], alpha=0.75, label=\"training\")\n",
    "axs[1, 0].hist(fake_production_df[\"petal length (cm)\"], alpha=0.50, label=\"production\")\n",
    "axs[1, 0].set_title(\"petal length (cm)\")\n",
    "axs[1, 0].legend()\n",
    "axs[1, 1].hist(training_df[\"petal width (cm)\"], alpha=0.75, label=\"training\")\n",
    "axs[1, 1].hist(fake_production_df[\"petal width (cm)\"], alpha=0.50, label=\"production\")\n",
    "axs[1, 1].set_title(\"petal width (cm)\")\n",
    "axs[1, 1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring drift for **univariate features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use Deepchecks' Dataset class to wrap our training and fake production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular import Dataset\n",
    "\n",
    "# Training dataset\n",
    "training_df[\"target\"] = iris.target\n",
    "training_dataset = Dataset(training_df, label=\"target\", cat_features=[])\n",
    "\n",
    "# Production dataset\n",
    "production_dataset = Dataset(fake_production_df, cat_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build a univariate drift detector that will check each feature in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import FeatureDrift\n",
    "\n",
    "check = FeatureDrift()\n",
    "result = check.run(training_dataset, production_dataset)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also add conditions (remember the Deepchecks concept hierarchy, i.e., suites, checks, and conditions).\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "check_cond = check.add_condition_drift_score_less_than(max_allowed_categorical_score=0.2,\n",
    "                                                       max_allowed_numeric_score=0.1)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring drift for **multivariate features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use Deepchecks to build a multiviariate drift detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.tabular.checks import MultivariateDrift\n",
    "\n",
    "check = MultivariateDrift()\n",
    "result = check.run(training_dataset, production_dataset)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring **prediction drift**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's experiment with Deepchecks' PredictionDrift detector.\n",
    "\n",
    "Here we need to load one of our trained models and use it to make predictions on the fake production data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from deepchecks.tabular.checks import PredictionDrift\n",
    "\n",
    "with open('../models/SVC_model.pkl', 'rb') as file:\n",
    "    model_wrapper = pickle.load(file)\n",
    "\n",
    "check = PredictionDrift()\n",
    "result = check.run(training_dataset, production_dataset, model=model_wrapper[\"model\"])\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "- [Made With ML > Monitoring Machine Learning Systems > Measuring Drift](https://madewithml.com/courses/mlops/monitoring/#measuring-drift)\n",
    "- [Alibi Detect Docs > Examples > Categorical and mixed type data drift detection on income prediction](https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastapi_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9f57285dff217c6b4ab697a45e2770d8c172cffea4368f6a4ca9b7811bbf7a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
